# Papers-Stack
Keeping a track of research papers I've read.<br>
... Guilty of not pushing the recently read on the Top!

[Articles-Collection](https://vdivakar.github.io/Articles-Collection/)
<br>
<br>
Keys -> ||
‚úÖ : Done reading ||
üìñ : In progress ||
üö´ : Dropped ||



|    |    | Paper Name                                                                                                                                                                                                                                                                                                 | Notes        | Link                                                                                                         | Year       |
| -- | -- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------ | ---------- |
| 1  | ‚úÖ  | WaveNet: A Generative Model for Raw Audio                                                                                                                                                                                                                                                                  | notes        | [arxiv](https://arxiv.org/abs/1609.03499)                                                                    | 2016       |
|    |    | Causal conv. layers with dilation. Autoregression model. Sequential inference                                                                                                                                                                                                                              |              |                                                                                                              |            |
| 2  | ‚úÖ  | Fast Wavenet Generation Algorithm                                                                                                                                                                                                                                                                          | notes        | [arxiv](https://arxiv.org/abs/1611.09482)                                                                    | 2016       |
|    |    | WaveNet improvement. O(2 L) -> O(L). Still sequential though.¬†<br>Use queues to push & pop already computed states at each layer                                                                                                                                                                           |              |                                                                                                              |            |
| 3  | ‚úÖ  | Parallel WaveNet: Fast High-Fidelity Speech Synthesis                                                                                                                                                                                                                                                      |              | [arxiv](https://arxiv.org/abs/1711.10433)                                                                    | 2017       |
|    | ‚¨áÔ∏è | Probability Density Distillation - Teacher + student based architecture. Marries efficient training of Wavenet with efficient IAF for sampling. Sampling is parallel here for realtime synthesis.<br>‚úîÔ∏èmedium - An Explanation of Discretized Logistic Mixture Likelihood¬†¬†<br>‚úîÔ∏è vimeo - Parallel WaveNet |              |                                                                                                              |            |
| 4  | üìï | Improved Variational Inference with Inverse Autoregressive Flow                                                                                                                                                                                                                                            |              | [arxiv](https://arxiv.org/abs/1606.04934)                                                                    | 2016       |
|    | ‚¨áÔ∏è | ‚≠ê ‚úîÔ∏è Introduction to Normalizing Flows (ECCV2020 Tutorial)                                                                                                                                                                                                                                                 |              | [video](https://www.youtube.com/watch?v=u3vVyFVU_lI)                                                         |            |
| 5  | ‚úÖ  | Deep Unsupervised Learning UC Berkeley lectures                                                                                                                                                                                                                                                            |              | [course](https://sites.google.com/view/berkeley-cs294-158-sp20/home)                                         |            |
|    |    | ‚úîÔ∏è L1 - Introduction -> Types: 1. Generative models 2. Self-supervised models                                                                                                                                                                                                                              |              | 01:10:00                                                                                                     |            |
|    |    | ‚úîÔ∏è L2 - Autoregressive Models -> histogram. parameterized distribution. 1.)RNN based 2.)Masking based. 2.1)MADE 2.2)Masked ConvNets                                                                                                                                                                        |              | 02:27:23                                                                                                     |            |
|    |    | ‚úîÔ∏è L3 - Flow Models -> Model output != p\_theta(x); instead z=f\_theta(x). z comes from a prob dist. Sampling is inverse of f\_inverse\_theta(x).¬†<br>\-> Autoregressive Flows:- Fast training; Slow sampling¬†<br>\-> Inverse Autoregressive Flow:- Slow training; Fast Sampling                           |              | 01:56:53                                                                                                     |            |
| 6  | ‚úÖ  | ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech                                                                                                                                                                                                                                            |              | [arxiv](https://arxiv.org/abs/1807.07281)                                                                    | 2018       |
| \- | \- | \--------------------                                                                                                                                                                                                                                                                                      | \---         | \---                                                                                                         | \--        |
| 7  | ‚úÖ  | Deep Photo Enhancer: unpaired learning for Image Enhancement using GANS                                                                                                                                                                                                                                    | CPVR         | [arxiv](https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.pdf) | 2018       |
|    |    | Cycle gan extension; individual BN for x->y' & x'->y''; adaptive weighting for WGAN                                                                                                                                                                                                                        |              |                                                                                                              |            |
| 8  | ‚úÖ  | AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE                                                                                                                                                                                                                                 | Google Brain | [arxiv](https://arxiv.org/abs/2010.11929)                                                                    | ICRL, 2021 |
|    |    | Vision Transformer (ViT) - sequence of img patches to Transformer. Less computation than ResNets. Training on large data trumps inductive bias in CNNs and outperforms.                                                                                                                                    |              |
